{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9568078f",
   "metadata": {},
   "source": [
    "### Clustring with Apache Spark\n",
    "Clustring is an unsupervised learning algorithm which groups similar data points together in clusters. It discovers the patterns based on the similarties.\n",
    "**Applications of Clustring**\n",
    "- Customer Segmentation\n",
    "- Anomly Detection\n",
    "- Image Segmentation (Computer Vision)\n",
    "- Recommendation System\n",
    "\n",
    "**Steps for implemntation of clustring algorithm using SparkML**\n",
    "1. Import libraries\n",
    "2. Start spark session\n",
    "3. Load data into spark dataframe\n",
    "4. Select features you wanna to use for clustring\n",
    "5. Assemble the features into one vector\n",
    "6. Train the K-mean model\n",
    "7. Make predictions\n",
    "8. Show the cluster assignments\n",
    "9. Again, Don't forget stop spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77377e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.clustering import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "531320b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/12/10 15:40:17 WARN Utils: Your hostname, omar, resolves to a loopback address: 127.0.1.1; using 192.168.1.4 instead (on interface wlo1)\n",
      "25/12/10 15:40:17 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/10 15:40:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Create Spark Session\n",
    "spark = SparkSession.builder.appName('Clustring with Apache Spark').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b4bc1b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file already exists\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "fileName='customers.csv'\n",
    "if test -f  data/$fileName; then\n",
    "    echo 'file already exists'\n",
    "else\n",
    "    echo 'Downloading  the file'\n",
    "    if wget -d  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0231EN-SkillsNetwork/datasets/customers.csv; then\n",
    "        echo 'file is downloaded successfully'\n",
    "        mv customers.csv data/\n",
    "    else\n",
    "        echo \"Something  Wrong Happened\"\n",
    "    fi\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abf3d15b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Fresh_Food: integer (nullable = true)\n",
      " |-- Milk: integer (nullable = true)\n",
      " |-- Grocery: integer (nullable = true)\n",
      " |-- Frozen_Food: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Load the data into spark dataframe\n",
    "df = spark.read.csv('data/customers.csv', header=True,inferSchema=True)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd9bb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'size of data is {df.count()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2515a41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dda8bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assemble features\n",
    "assembler = VectorAssembler(inputCols=df.columns, outputCol='features')\n",
    "df = assembler.transform(df)\n",
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42416dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af384ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initiate K-mean algorithm\n",
    "#You should decide the number of clusters the data should belong to\n",
    "clusters = 3\n",
    "k_mean = KMeans(k=clusters)\n",
    "\n",
    "#Train the model\n",
    "model = k_mean.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b100e1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make Predictions\n",
    "predictions = model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2858a1e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-------+-----------+--------------------+----------+\n",
      "|Fresh_Food|Milk|Grocery|Frozen_Food|            features|prediction|\n",
      "+----------+----+-------+-----------+--------------------+----------+\n",
      "|     12669|9656|   7561|        214|[12669.0,9656.0,7...|         0|\n",
      "|      7057|9810|   9568|       1762|[7057.0,9810.0,95...|         0|\n",
      "|      6353|8808|   7684|       2405|[6353.0,8808.0,76...|         0|\n",
      "|     13265|1196|   4221|       6404|[13265.0,1196.0,4...|         0|\n",
      "|     22615|5410|   7198|       3915|[22615.0,5410.0,7...|         2|\n",
      "+----------+----+-------+-----------+--------------------+----------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "predictions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d54c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|         1|   49|\n",
      "|         2|   60|\n",
      "|         0|  331|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#There are three clusters and the number of observations belong to each cluster\n",
    "predictions.groupBy('prediction').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4dca7486",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark-env (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65d889c1",
   "metadata": {},
   "source": [
    "## *Practice Project on Machine Learning using Apache Spark*\n",
    "The main goal in this project is to predict the sound level based on other columns. I will do that using SparkML to implement the machine learning algorithms.\n",
    "\n",
    "#### What is gonnna be done to perform the task?\n",
    "\n",
    "The project is splitted into four parts as the following:\n",
    "1. Perform ETL activities such as extracting data from csv file and load it to a spark dataframe, apply some transformation such as (removing duplicates and null values if exists and any necessary transformations) and finally store cleaned data in the parquet format.\n",
    "\n",
    "2. Create machine learning pipeline with three stages including regression stage. The pipeline is the backbone of the model development process.\n",
    "\n",
    "3. Evaluate the model through evaluation metrices according to which algorithm we used.\n",
    "4. The last part is to persist the model on local machine and allow loading it again to predict new real-world data. It helps in reusability, portability for future use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bfa039",
   "metadata": {},
   "source": [
    "### Part 1 (Clean and Transform data)\n",
    "\n",
    "In this part, We will start with makeing the dataset ready for the model. This will done throughout the following:\n",
    "1. Load the data from csv file.\n",
    "2. Understand the dataset (I think it's the most important step in any data-based projects).\n",
    "3. Define where is the problems (Are there missing values, duplicates or any other defectives).\n",
    "4. Clean the data using apache spark dataframe.\n",
    "5. Do any necessary transformations.\n",
    "6. Load cleaned and transformed data in parquet format file for future usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83307fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import os\n",
    "import findspark\n",
    "from pyspark.sql import SparkSession, functions as F, types as T\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.pipeline import PipelineModel\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b6bd9c",
   "metadata": {},
   "source": [
    "Download the data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a3b308ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "fileName=\"mpg-raw.csv\"\n",
    "\n",
    "if ! test -d data;then\n",
    "    mkdir data\n",
    "fi\n",
    "\n",
    "if ! test -f  data/$fileName;then\n",
    "    wget -P data/ https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-BD0231EN-Coursera/datasets/mpg-raw.csv\n",
    "else\n",
    "    echo \"File already exists\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8733ae09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Spark instance\n",
    "findspark.init()\n",
    "spark = SparkSession.builder.appName('Practice Project').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "85797637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- MPG: double (nullable = true)\n",
      " |-- Cylinders: integer (nullable = true)\n",
      " |-- Engine Disp: double (nullable = true)\n",
      " |-- Horsepower: integer (nullable = true)\n",
      " |-- Weight: integer (nullable = true)\n",
      " |-- Accelerate: double (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Load the data file to spark dataframe\n",
    "df = spark.read.csv('data/mpg-raw.csv',header=True,inferSchema=True)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2482b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|  Origin|count|\n",
      "+--------+-----+\n",
      "|European|   70|\n",
      "|    NULL|    1|\n",
      "|Japanese|   88|\n",
      "|American|  247|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#One null apears means that data contains nulls\n",
    "df.groupBy('Origin').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "49cfa1c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "406"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_row_count = df.count()\n",
    "total_row_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2a3924a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "392"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Drop duplicates\n",
    "df  = df.drop_duplicates()\n",
    "no_duplicates_row_count = df.count()\n",
    "no_duplicates_row_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b1820d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+-----------+----------+------+----------+----+--------+-----+\n",
      "| MPG|Cylinders|Engine Disp|Horsepower|Weight|Accelerate|Year|  Origin|nulls|\n",
      "+----+---------+-----------+----------+------+----------+----+--------+-----+\n",
      "|33.5|        4|      151.0|        90|  2556|      13.2|  79|    NULL|    1|\n",
      "|NULL|        4|       97.0|        67|  2065|      17.8|  81|Japanese|    1|\n",
      "|32.0|        4|       83.0|      NULL|  2003|      19.0|  74|Japanese|    1|\n",
      "|32.0|        4|       NULL|        96|  2665|      13.9|  82|Japanese|    1|\n",
      "|28.0|        4|      116.0|        90|  2123|      NULL|  71|European|    1|\n",
      "|30.7|        6|      145.0|        76|  NULL|      19.6|  81|European|    1|\n",
      "|30.0|     NULL|      135.0|        84|  2385|      12.9|  81|American|    1|\n",
      "+----+---------+-----------+----------+------+----------+----+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#This represent which columns contain null values\n",
    "expr = sum(F.when(F.col(x).isNull(),1).otherwise(0)for x in df.columns)\n",
    "df.withColumn('nulls', expr).filter(F.col('nulls') > 0).orderBy('nulls',ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d8a79b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-----------+----------+------+----------+----+------+-----+\n",
      "|MPG|Cylinders|Engine Disp|Horsepower|Weight|Accelerate|Year|Origin|nulls|\n",
      "+---+---------+-----------+----------+------+----------+----+------+-----+\n",
      "+---+---------+-----------+----------+------+----------+----+------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "385"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Drop Null Values\n",
    "df = df.dropna()\n",
    "no_null_row_count = df.count()\n",
    "df.withColumn('nulls', expr).filter(F.col('nulls') > 0).orderBy('nulls',ascending=False).show()\n",
    "#Nulls are dropped\n",
    "no_null_row_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "693a11d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MPG',\n",
       " 'Cylinders',\n",
       " 'Engine_Disp',\n",
       " 'Horsepower',\n",
       " 'Weight',\n",
       " 'Accelerate',\n",
       " 'Year',\n",
       " 'Origin']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Rename Columns\n",
    "df = df.withColumnRenamed('Engine Disp','Engine_Disp')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a28a524",
   "metadata": {},
   "source": [
    "I think data is cleaned now. Let's save it in parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "64c97f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.mode('overwrite').parquet('data/mpg-cleaned.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b9dcb936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row count of original data = 406\n",
      "Row count of data after dropping duplicates rows = 392\n",
      "Row count of after dropping rows contain null values = 385\n",
      "mpg-cleaned.parquet exists: True\n"
     ]
    }
   ],
   "source": [
    "print(f\"Row count of original data = {total_row_count}\")\n",
    "print(f\"Row count of data after dropping duplicates rows = {no_duplicates_row_count}\")\n",
    "print(f\"Row count of after dropping rows contain null values = {no_null_row_count}\")\n",
    "print(f\"mpg-cleaned.parquet exists: {os.path.isdir('data/mpg-cleaned.parquet/')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acbf140",
   "metadata": {},
   "source": [
    "### Part 2 (Build Machine Learning Pipeline)\n",
    "\n",
    "In this part, We will start building the pipeline stages that implement the machine learning model and that will done throughout the following:\n",
    "1. Create `StringIndexer` stage.\n",
    "2. Create `VectorAssembler` stage.\n",
    "3. Create `StandardScaler` stage.\n",
    "4. Define model creation stage.\n",
    "5. Build the pipeline from the above stages.\n",
    "6. Split the dataset into training and test sets.\n",
    "7. Fit the model using training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6f6a7b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+-----------+----------+------+----------+----+--------+\n",
      "| MPG|Cylinders|Engine_Disp|Horsepower|Weight|Accelerate|Year|  Origin|\n",
      "+----+---------+-----------+----------+------+----------+----+--------+\n",
      "|34.0|        4|      108.0|        70|  2245|      16.9|  82|Japanese|\n",
      "|27.0|        4|      112.0|        88|  2640|      18.6|  82|American|\n",
      "|22.0|        6|      250.0|       105|  3353|      14.5|  76|American|\n",
      "|29.0|        4|       98.0|        83|  2219|      16.5|  74|European|\n",
      "|41.5|        4|       98.0|        76|  2144|      14.7|  80|European|\n",
      "+----+---------+-----------+----------+------+----------+----+--------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "bbc343d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a string indexer of Origin attribute\n",
    "indexer = StringIndexer(inputCol='Origin',outputCol='Origin_Indexer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7e86c879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|  Origin|Index|\n",
      "+--------+-----+\n",
      "|European|  2.0|\n",
      "|Japanese|  1.0|\n",
      "|American|  0.0|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Print each Origin category with its index\n",
    "indexer.fit(df).transform(df).groupBy('Origin').agg(\n",
    "    F.median(F.col('Origin_Indexer')).alias('Index')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "98febd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine necessary features in one vector in column called features\n",
    "#define necessary features\n",
    "input_cols = df.columns[1:len(df.columns)-1]\n",
    "assembler = VectorAssembler(inputCols=input_cols, outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "adeb1e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+----------+------+----------+----+----------------------------------+\n",
      "|Cylinders|Engine_Disp|Horsepower|Weight|Accelerate|Year|features                          |\n",
      "+---------+-----------+----------+------+----------+----+----------------------------------+\n",
      "|4        |108.0      |70        |2245  |16.9      |82  |[4.0,108.0,70.0,2245.0,16.9,82.0] |\n",
      "|4        |112.0      |88        |2640  |18.6      |82  |[4.0,112.0,88.0,2640.0,18.6,82.0] |\n",
      "|6        |250.0      |105       |3353  |14.5      |76  |[6.0,250.0,105.0,3353.0,14.5,76.0]|\n",
      "|4        |98.0       |83        |2219  |16.5      |74  |[4.0,98.0,83.0,2219.0,16.5,74.0]  |\n",
      "|4        |98.0       |76        |2144  |14.7      |80  |[4.0,98.0,76.0,2144.0,14.7,80.0]  |\n",
      "+---------+-----------+----------+------+----------+----+----------------------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "assembler.transform(df).select(input_cols).show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95185f25",
   "metadata": {},
   "source": [
    "Now, these features must be scaled to prevent model bias, So we will scale these features using `StandardScaler`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "1e33b2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(inputCol='features',outputCol='scaled_features')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68042126",
   "metadata": {},
   "source": [
    "We need to define the  model creation stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "73a9d215",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(featuresCol='scaled_features', labelCol='MPG')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb62c66",
   "metadata": {},
   "source": [
    "Then, Build the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "46bff91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[indexer,assembler, scaler,lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba04fa57",
   "metadata": {},
   "source": [
    "Now the time we need to split our dataset into training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "6860acbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "(training_data, testing_data)  = df.randomSplit([0.7,0.3],seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5c0f48",
   "metadata": {},
   "source": [
    "Let's train the model using `training_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a9bd8a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/13 17:16:29 WARN Instrumentation: [a9d22bbe] regParam is zero, which might cause numerical instability and overfitting.\n"
     ]
    }
   ],
   "source": [
    "pipeline_model = pipeline.fit(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "25863a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline Stages\n",
      "-------------------------\n",
      "Stage 1: StringIndexer\n",
      "Stage 2: VectorAssembler\n",
      "Stage 3: StandardScaler\n",
      "Stage 4: LinearRegression\n",
      "-------------------------\n",
      "The Label Column: MPG\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "print(f\"Pipeline Stages\")\n",
    "print('-'*25)\n",
    "for i,value in enumerate([str(x).split('_')[0] for x in pipeline.getStages()]):\n",
    "    print(f\"Stage {i+1}: {value}\")\n",
    "\n",
    "print('-'*25)\n",
    "print(f\"The Label Column: {lr.getLabelCol()}\")\n",
    "print('-'*25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1b35ed",
   "metadata": {},
   "source": [
    "### Part 3 (Evaluation)\n",
    "\n",
    "The next step is evaluation which are predict the testing data using the trained model by the training data and  then finally compute the evaluation metrices for the model to ensure the quality of the  prediction. We do that in the following order:\n",
    "1. Predict the testing_data using the trained model.\n",
    "\n",
    "2. Generate Evaluation metrices of the model to ensure the quality of prediction. Metrices used in this evalutation:\n",
    "    - Mean Square Error (MSE).\n",
    "    - Root Mean Square Error  (RMSE).\n",
    "    - Mean Absolute Error (MAE).\n",
    "    - R-Squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "da3395ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MPG',\n",
       " 'Cylinders',\n",
       " 'Engine_Disp',\n",
       " 'Horsepower',\n",
       " 'Weight',\n",
       " 'Accelerate',\n",
       " 'Year',\n",
       " 'Origin',\n",
       " 'Origin_Indexer',\n",
       " 'features',\n",
       " 'scaled_features',\n",
       " 'prediction']"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = pipeline_model.transform(testing_data)\n",
    "predictions.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "859eb656",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator= RegressionEvaluator(predictionCol='prediction',labelCol='MPG', metricName='mse')\n",
    "mse = evaluator.evaluate(predictions)\n",
    "evaluator= RegressionEvaluator(predictionCol='prediction',labelCol='MPG', metricName='r2')\n",
    "r2 = evaluator.evaluate(predictions)\n",
    "evaluator= RegressionEvaluator(predictionCol='prediction',labelCol='MPG', metricName='rmse')\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "evaluator= RegressionEvaluator(predictionCol='prediction',labelCol='MPG', metricName='mae')\n",
    "mae = evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "4b3035c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 12.22674583557129\n",
      "RMSE = 3.4966763984634452\n",
      "MAE = 2.8457151130135854\n",
      "R-Squared = 0.8018737394895717\n"
     ]
    }
   ],
   "source": [
    "print(f\"MSE = {mse}\")\n",
    "print(f\"RMSE = {rmse}\")\n",
    "print(f\"MAE = {mae}\")\n",
    "print(f\"R-Squared = {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "8007c8fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept of the linear regression model = -17.37\n"
     ]
    }
   ],
   "source": [
    "#Print the Intercept of the linear regression model\n",
    "linearRegressionModel = pipeline_model.stages[-1]  #Because it's the last stage in the pipeline  model\n",
    "print(f\"Intercept of the linear regression model = {round(linearRegressionModel.intercept,2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7871980",
   "metadata": {},
   "source": [
    "### Part 4 (Make the model persist)\n",
    "\n",
    "Making the model persist is so important for reusability and portability and time consuming wise because it allows you save the trained model in you local machine and you can use it again on new real-world datasets by just loading the model and predict the new data you have.\n",
    "\n",
    "This will be done through the following steps:\n",
    "1. Save the trained mode in the physical disk on the local machine.\n",
    "2. Load the model again.\n",
    "3. Use it to predict new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "93855788",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "if ! test -d saved_models; then\n",
    "    mkdir  saved_models\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "749d5112",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the model\n",
    "pipeline_model.write().overwrite().save('saved_models/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "e8e615e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the model\n",
    "loaded_model = PipelineModel.load('saved_models/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "d96727bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the loaded model to predict testing data\n",
    "newPerdictions = loaded_model.transform(testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "304e77c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+-----------+----------+------+----------+----+--------+--------------+--------------------+--------------------+------------------+\n",
      "| MPG|Cylinders|Engine_Disp|Horsepower|Weight|Accelerate|Year|  Origin|Origin_Indexer|            features|     scaled_features|        prediction|\n",
      "+----+---------+-----------+----------+------+----------+----+--------+--------------+--------------------+--------------------+------------------+\n",
      "|10.0|        8|      360.0|       215|  4615|      14.0|  70|American|           0.0|[8.0,360.0,215.0,...|[4.81279869941784...| 6.960764577508346|\n",
      "|11.0|        8|      429.0|       208|  4633|      11.0|  72|American|           0.0|[8.0,429.0,208.0,...|[4.81279869941784...| 8.545911819807522|\n",
      "|12.0|        8|      350.0|       180|  4499|      12.5|  73|American|           0.0|[8.0,350.0,180.0,...|[4.81279869941784...|10.226709705747677|\n",
      "|12.0|        8|      383.0|       180|  4955|      11.5|  71|American|           0.0|[8.0,383.0,180.0,...|[4.81279869941784...| 5.446415257213175|\n",
      "|13.0|        8|      302.0|       129|  3169|      12.0|  75|American|           0.0|[8.0,302.0,129.0,...|[4.81279869941784...|21.430212400590392|\n",
      "+----+---------+-----------+----------+------+----------+----+--------+--------------+--------------------+--------------------+------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "newPerdictions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "2435cb0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 12.22674583557129\n",
      "RMSE = 3.4966763984634452\n",
      "MAE = 2.8457151130135854\n",
      "R-Squared = 0.8018737394895717\n"
     ]
    }
   ],
   "source": [
    "evaluator= RegressionEvaluator(predictionCol='prediction',labelCol='MPG', metricName='mse')\n",
    "mse = evaluator.evaluate(newPerdictions)\n",
    "evaluator= RegressionEvaluator(predictionCol='prediction',labelCol='MPG', metricName='r2')\n",
    "r2 = evaluator.evaluate(newPerdictions)\n",
    "evaluator= RegressionEvaluator(predictionCol='prediction',labelCol='MPG', metricName='rmse')\n",
    "rmse = evaluator.evaluate(newPerdictions)\n",
    "evaluator= RegressionEvaluator(predictionCol='prediction',labelCol='MPG', metricName='mae')\n",
    "mae = evaluator.evaluate(newPerdictions)\n",
    "\n",
    "\n",
    "#Displaying the results\n",
    "print(f\"MSE = {mse}\")\n",
    "print(f\"RMSE = {rmse}\")\n",
    "print(f\"MAE = {mae}\")\n",
    "print(f\"R-Squared = {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "269105b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringIndexerModel: uid=StringIndexer_f760d9b7fc4b, handleInvalid=error,\n",
       " VectorAssembler_b6a02e2a0178,\n",
       " StandardScalerModel: uid=StandardScaler_f4c5091385f1, numFeatures=6, withMean=false, withStd=true,\n",
       " LinearRegressionModel: uid=LinearRegression_a291628b5dca, numFeatures=6]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loaded model stages\n",
    "loaded_model.stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "a3114ac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Cylinders', 'Engine_Disp', 'Horsepower', 'Weight', 'Accelerate', 'Year']"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputColumns = loaded_model.stages[1].getInputCols() \n",
    "inputColumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "bfa4b1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark-env (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4d754bd",
   "metadata": {},
   "source": [
    "## Pyspark Project\n",
    "This is a practice project which i apply what i learn while learning apache spark (pyspark and sparksql). I assume that i am a data engineer and this is a task to perform tasks such as reading, analyzing, transforming and loading  using pyspark and sparksql.\n",
    "### Objectives\n",
    "- Read datasets\n",
    "- Do some transformation\n",
    "- Load data in different sources (Hive data warehouse and HDFS (Hadoop Distributed File system))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32381942",
   "metadata": {},
   "source": [
    "### Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "eac71a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-12-02 19:11:14--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/data/dataset1.csv\n",
      "Resolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 169.63.118.104\n",
      "Connecting to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|169.63.118.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4115 (4.0K) [text/csv]\n",
      "Saving to: ‘dataset1.csv’\n",
      "\n",
      "dataset1.csv        100%[===================>]   4.02K  --.-KB/s    in 0s      \n",
      "\n",
      "2025-12-02 19:11:15 (1.01 GB/s) - ‘dataset1.csv’ saved [4115/4115]\n",
      "\n",
      "--2025-12-02 19:11:15--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/data/dataset2.csv\n",
      "Resolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 169.63.118.104\n",
      "Connecting to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|169.63.118.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2688 (2.6K) [text/csv]\n",
      "Saving to: ‘dataset2.csv’\n",
      "\n",
      "dataset2.csv        100%[===================>]   2.62K  --.-KB/s    in 0s      \n",
      "\n",
      "2025-12-02 19:11:16 (558 MB/s) - ‘dataset2.csv’ saved [2688/2688]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/data/dataset1.csv  \n",
    "!wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/data/dataset2.csv  \n",
    "!rm -r data\n",
    "!mkdir data\n",
    "!mv dataset1.csv data/dataset1.csv\n",
    "!mv dataset2.csv data/dataset2.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5a165370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "import findspark\n",
    "from pyspark.sql.types import StructField, StructType, IntegerType, DateType, StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8d479342",
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "65b183c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8047191c",
   "metadata": {},
   "source": [
    "### Why the following cell is important\n",
    "Because the DateType() in pyspark support only the following format `yyyy-MM-dd` without the following the date column after reading the file becoming NULL becuase spark.type DateType cannot read it. So the following cell transform the original csv file to be in the correct format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cb504095",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = pd.read_csv('data/dataset1.csv')\n",
    "d2 = pd.read_csv('data/dataset2.csv')\n",
    "def transform_date (x):\n",
    "    l= x.split(\"/\")\n",
    "    year=l[len(l)-1]\n",
    "    month= l[1]\n",
    "    day=l[0]\n",
    "    return f\"{year}-{month}-{day}\"\n",
    "\n",
    "d1['date_column'] = d1['date_column'].apply(transform_date)\n",
    "d2['transaction_date'] = d2['transaction_date'].apply(transform_date)\n",
    "\n",
    "\n",
    "d1.to_csv('data/dataset1.csv',index=False)\n",
    "d2.to_csv('data/dataset2.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141200c2",
   "metadata": {},
   "source": [
    "### Defining the schema of the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6749614c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df1_schema = StructType(\n",
    "    [\n",
    "        StructField('customer_id', IntegerType(),True),\n",
    "        StructField('date_column', DateType(),True),\n",
    "        StructField('amount', IntegerType(),True),\n",
    "        StructField('description', StringType(),True),\n",
    "        StructField('location', StringType(),True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "df2_schema = StructType(\n",
    "    [\n",
    "        StructField('customer_id', IntegerType(),True),\n",
    "        StructField('transaction_date', DateType(),True),\n",
    "        StructField('value', IntegerType(),True),\n",
    "        StructField('note', StringType(),True)\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "df1 = spark.read.csv('data/dataset1.csv',header=True, schema=df1_schema)\n",
    "df2 = spark.read.csv('data/dataset2.csv',header=True, schema=df2_schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1c803102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema of DataFrame 1\n",
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- date_column: date (nullable = true)\n",
      " |-- amount: integer (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      "\n",
      "Schema of DataFrame 2\n",
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- transaction_date: date (nullable = true)\n",
      " |-- value: integer (nullable = true)\n",
      " |-- note: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Schema of DataFrame 1\")\n",
    "df1.printSchema()\n",
    "print(f\"Schema of DataFrame 2\")\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0271ea",
   "metadata": {},
   "source": [
    "### Add new columns to each dataframe\n",
    "1. Add `year` column to dataframe 1\n",
    "2. Add `quarter` column to dataframe 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "056ef39d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+------+-----------+--------+----+\n",
      "|customer_id|date_column|amount|description|location|Year|\n",
      "+-----------+-----------+------+-----------+--------+----+\n",
      "|          1| 2022-01-01|  5000| Purchase A| Store A|2022|\n",
      "|          2| 2022-02-15|  1200| Purchase B| Store B|2022|\n",
      "+-----------+-----------+------+-----------+--------+----+\n",
      "only showing top 2 rows\n",
      "+-----------+----------------+-----+------+-------+\n",
      "|customer_id|transaction_date|value|  note|Quarter|\n",
      "+-----------+----------------+-----+------+-------+\n",
      "|          1|      2022-01-01| 1500|Note 1|      1|\n",
      "|          2|      2022-02-15| 2000|Note 2|      1|\n",
      "+-----------+----------------+-----+------+-------+\n",
      "only showing top 2 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/02 19:05:36 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: customer_id, transaction_date, value, notes\n",
      " Schema: customer_id, transaction_date, value, note\n",
      "Expected: note but found: notes\n",
      "CSV file: file:///home/omar/me/Data-Engineering/big-data/Final-Projects/data/dataset2.csv\n"
     ]
    }
   ],
   "source": [
    "df1 = df1.withColumn('Year', F.year(F.col('date_column')))\n",
    "df2 = df2.withColumn('Quarter', F.quarter(F.col('transaction_date')))\n",
    "\n",
    "df1.show(2)\n",
    "df2.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb2c829",
   "metadata": {},
   "source": [
    "### Rename Columns in DataFrames\n",
    "1. In dataframe 1, rename `amount` to be `transaction_amount`\n",
    "2. In dataframe 2, rename `value` to be `transaction_value`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2fec7802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['customer_id', 'date_column', 'transaction_amount', 'description', 'location', 'Year']\n",
      "['customer_id', 'transaction_date', 'transaction_value', 'note', 'Quarter']\n"
     ]
    }
   ],
   "source": [
    "df1 = df1.withColumnRenamed('amount','transaction_amount')\n",
    "df2 = df2.withColumnRenamed('value','transaction_value')\n",
    "\n",
    "print(df1.columns)\n",
    "print(df2.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b785bc",
   "metadata": {},
   "source": [
    "### Drop unnecessary Columns\n",
    "1. In dataframe 1, drop `description` and `location`\n",
    "2. In dataframe 2, drop `note`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "59e93824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['customer_id', 'date_column', 'transaction_amount', 'Year']\n",
      "['customer_id', 'transaction_date', 'transaction_value', 'Quarter']\n"
     ]
    }
   ],
   "source": [
    "df1 = df1.drop('description','location')\n",
    "df2 = df2.drop('note')\n",
    "\n",
    "print(df1.columns)\n",
    "print(df2.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bbecf7",
   "metadata": {},
   "source": [
    "### Join both dataframes based on `customer_id` with inner join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6a335d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df = df1.join(df2,on='customer_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2d051b8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['customer_id',\n",
       " 'date_column',\n",
       " 'transaction_amount',\n",
       " 'Year',\n",
       " 'transaction_date',\n",
       " 'transaction_value',\n",
       " 'Quarter']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ffddd6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+------------------+----+----------------+-----------------+-------+\n",
      "|customer_id|date_column|transaction_amount|Year|transaction_date|transaction_value|Quarter|\n",
      "+-----------+-----------+------------------+----+----------------+-----------------+-------+\n",
      "|          1| 2022-01-01|              5000|2022|      2022-01-01|             1500|      1|\n",
      "|          2| 2022-02-15|              1200|2022|      2022-02-15|             2000|      1|\n",
      "|          3| 2022-03-20|               800|2022|      2022-03-20|             1000|      1|\n",
      "|          4| 2022-04-10|              3000|2022|      2022-04-10|             2500|      2|\n",
      "|          5| 2022-05-05|              6000|2022|      2022-05-05|             1800|      2|\n",
      "+-----------+-----------+------------------+----+----------------+-----------------+-------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "joined_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f968d2d",
   "metadata": {},
   "source": [
    "### Do some filteration tasks on the joined dataframe\n",
    "- filter records which are have `transaction_amount` greater than 1000 and store the result in new dataframe `filtered-df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "94d23961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+------------------+----+----------------+-----------------+-------+\n",
      "|customer_id|date_column|transaction_amount|Year|transaction_date|transaction_value|Quarter|\n",
      "+-----------+-----------+------------------+----+----------------+-----------------+-------+\n",
      "|         22| 2023-10-30|              1200|2023|      2023-10-30|              700|      4|\n",
      "|         34| 2024-10-30|              1200|2024|      2024-10-30|              700|      4|\n",
      "|          2| 2022-02-15|              1200|2022|      2022-02-15|             2000|      1|\n",
      "|         70| 2027-10-30|              1200|2027|      2027-10-30|              700|      4|\n",
      "|         46| 2025-10-30|              1200|2025|      2025-10-30|              700|      4|\n",
      "+-----------+-----------+------------------+----+----------------+-----------------+-------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "filtered_df = joined_df.filter(joined_df['transaction_amount'] > 1000)\n",
    "filtered_df.orderBy('transaction_amount',ascending=True).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "85590753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+\n",
      "|transaction_amount|count|\n",
      "+------------------+-----+\n",
      "|              6000|    1|\n",
      "|              5500|    7|\n",
      "|              5000|    1|\n",
      "|              4800|    1|\n",
      "|              4500|    1|\n",
      "|              4200|    8|\n",
      "|              3500|    1|\n",
      "|              3200|    7|\n",
      "|              3000|    1|\n",
      "|              2600|    8|\n",
      "|              2400|    7|\n",
      "|              2200|    1|\n",
      "|              1800|    8|\n",
      "|              1500|    7|\n",
      "|              1200|    8|\n",
      "+------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Data is filitered correctly\n",
    "filtered_df.groupBy('transaction_amount').agg(\n",
    "    F.count('customer_id').alias('count')\n",
    ").orderBy('transaction_amount',ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b913ac3f",
   "metadata": {},
   "source": [
    "### Aggregate the dataframe by customers and calculate the total `transaction_amount` of each customer who his `transaction_amount` is greater than 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fb85835c",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_amount_per_customer = filtered_df.groupBy('customer_id').agg(\n",
    "    F.sum('transaction_amount').alias('Total_Transaction_Amount')\n",
    ").orderBy('customer_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "961b587a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------------+\n",
      "|customer_id|Total_Transaction_Amount|\n",
      "+-----------+------------------------+\n",
      "|          1|                    5000|\n",
      "|          2|                    1200|\n",
      "|          4|                    3000|\n",
      "|          5|                    6000|\n",
      "|          6|                    4500|\n",
      "+-----------+------------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "total_amount_per_customer.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090b4341",
   "metadata": {},
   "source": [
    "### Writing data\n",
    "- `total_amount_per_customer` dataframe in a Hive table named `customer_totals`\n",
    "- `filtered_df` in HDFS in parquet format called `filtered_data`\n",
    "\n",
    "**Why parquet??**\n",
    "\n",
    "It's a columnar storage file format that is efficient for big data processing because it stores data by columns instead of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "98d05f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Overwrite the table if exists\n",
    "total_amount_per_customer.write.mode('overwrite').saveAsTable('customer_totals')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2df64103",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.write.mode('overwrite').parquet('filtered-data.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b5e0d7",
   "metadata": {},
   "source": [
    "### Add new columns based on specific conditions\n",
    "- Add column `high_value` in df1 store **YES** if `transaction_amount` is greater than 5000 otherwise store **NO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0068e740",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.withColumn('high_value', F.when((df1['transaction_amount'] > 5000),'YES').otherwise('NO'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d5fc865c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+------------------+----+----------+\n",
      "|customer_id|date_column|transaction_amount|Year|high_value|\n",
      "+-----------+-----------+------------------+----+----------+\n",
      "|          1| 2022-01-01|              5000|2022|        NO|\n",
      "|          2| 2022-02-15|              1200|2022|        NO|\n",
      "|          3| 2022-03-20|               800|2022|        NO|\n",
      "|          4| 2022-04-10|              3000|2022|        NO|\n",
      "|          5| 2022-05-05|              6000|2022|       YES|\n",
      "+-----------+-----------+------------------+----+----------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "df1.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a59e288",
   "metadata": {},
   "source": [
    "### Calculate the average transaction value per quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2a79b8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_value_per_quarter = df2.groupBy('Quarter').agg(\n",
    "    F.round(F.avg('transaction_value'),2).alias('Average_Transactional_Value')\n",
    "    ).orderBy('Quarter')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec969bc",
   "metadata": {},
   "source": [
    "### Write the result `average_value_per_quarter` in Hive table called `quarterly_averages`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a0f4ee19",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_value_per_quarter.write.mode('overwrite').saveAsTable('quarterly_averages')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d34d662",
   "metadata": {},
   "source": [
    "###  Calculate the total transaction value per year\n",
    "- In first dataframe, compute the total transaction value per year and store it in data frame called `total_transaction_val`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3737a1a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['customer_id', 'date_column', 'transaction_amount', 'Year', 'high_value']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ac6195a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_transaction_val = df1.groupBy('Year').agg(\n",
    "    F.sum('transaction_amount').alias('Total_Transaction_Amount')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d520bf44",
   "metadata": {},
   "source": [
    "### Save the result in a csv file and store it in HDFS under the name of `total_value_per_year`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "24edab16",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_transaction_val.write.mode('overwrite').csv('total_value_per_year.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80a4e41",
   "metadata": {},
   "source": [
    "***Stay Tuned.***\n",
    "\n",
    "*Wish Me LUCK :)*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
